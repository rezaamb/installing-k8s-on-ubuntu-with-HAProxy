# ğŸš€ğŸš€ Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ú¯Ø§Ù…â€ŒØ¨Ù‡â€ŒÚ¯Ø§Ù… Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙ‚Ø±Ø§Ø± ÛŒÚ© Ø®ÙˆØ´Ù‡ Kubernetes Ø±ÙˆÛŒ Ubuntu Ø¨Ø§ HAProxy

Ø¨Ù‡ Ø±Ø§Ù‡Ù†Ù…Ø§ Ø¨Ø±Ø§ÛŒ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ ÛŒÚ© Ø®ÙˆØ´Ù‡ Kubernetes Ø¨Ø§ HAProxy Ø®ÙˆØ´â€ŒØ§ÙˆÙ…Ø¯ÛŒ! Ø§ÛŒÙ† Ø³Ù†Ø¯ Ø´Ù…Ø§ Ø±Ùˆ Ú¯Ø§Ù… Ø¨Ù‡ Ú¯Ø§Ù… Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ù‡ ØªØ§ HAProxy Ø±Ùˆ ØªÙ†Ø¸ÛŒÙ… Ú©Ø±Ø¯Ù‡ Ùˆ Ø¢Ø¯Ø±Ø³ API server Ø±Ùˆ Ø¯Ø±Ø³Øª Ø¨Ù‡â€ŒÚ©Ø§Ø± Ø¨Ú¯ÛŒØ±ÛŒ.
```Tested on Ubuntu 20.04, 22.04 and 24.04 âœ…```

âœ… Ø§ÛŒÙ† Ø±Ø§Ù‡Ù†Ù…Ø§ Ø±ÙˆÛŒ Ubuntu 20.04ØŒ 22.04 Ùˆ 24.04 ØªØ³Øª Ø´Ø¯Ù‡ Ùˆ Ù‡Ù…Ù‡ Ù…Ø±Ø§Ø­Ù„ Ø¨Ø§ Ø§ÛŒÙ† Ù†Ø³Ø®Ù‡â€ŒÙ‡Ø§ Ø³Ø§Ø²Ú¯Ø§Ø± Ù‡Ø³ØªÙ†Ø¯.
ğŸ“ Ù…Ù‚Ø¯Ù…Ù‡

 Ú©ÙˆØ¨Ø±Ù†ØªÛŒØ² ÛŒÚ© Ø³ÛŒØ³ØªÙ… Ù…ØªÙ†â€ŒØ¨Ø§Ø² Ø¨Ø±Ø§ÛŒ Ø§Ø±Ú©Ø³ØªØ±ÛŒØ´Ù† Ú©Ø§Ù†ØªÛŒÙ†Ø± Ø§Ø³Øª Ú©Ù‡ Ú©Ø§Ø±Ù‡Ø§ÛŒ DeploymentØŒ Ù…Ù‚ÛŒØ§Ø³â€ŒØ¯Ù‡ÛŒ Ùˆ Ù…Ø¯ÛŒØ±ÛŒØª Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ‡Ø§ Ø±Ùˆ Ø§ØªÙˆÙ…Ø§ØªÛŒÚ© Ù…ÛŒâ€ŒÚ©Ù†Ù‡. Ø§ÛŒÙ† Ù¾Ø±ÙˆÚ˜Ù‡ ØªÙˆØ³Ø· Ú¯ÙˆÚ¯Ù„ Ø´Ø±ÙˆØ¹ Ø´Ø¯ Ùˆ Ø­Ø§Ù„Ø§ Ø²ÛŒØ± Ù†Ø¸Ø± Ø¬Ø§Ù…Ø¹Ù‡ Ù…ØªÙ†â€ŒØ¨Ø§Ø² Ùˆ Cloud Native Computing Foundation Ø§Ø¯Ø§Ù…Ù‡ Ù¾ÛŒØ¯Ø§ Ù…ÛŒâ€ŒÚ©Ù†Ù‡.
Ø¨ÛŒØ§ÛŒØ¯ Ù…Ø±Ø­Ù„Ù‡ Ø¨Ù‡ Ù…Ø±Ø­Ù„Ù‡ Ù†ØµØ¨Ø´ Ú©Ù†ÛŒÙ… âœ”ï¸


1ï¸âƒ£ ØºÛŒØ±ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Swap Memory ğŸ›‘

 Ú©ÙˆØ¨Ø±Ù†ØªÛŒØ² Ø¨Ø±Ø§ÛŒ Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯ÛŒ Ú©Ø§Ø±Ù‡Ø§ØŒ Ù…Ù†Ø§Ø¨Ø¹ Ø³ÛŒØ³ØªÙ… (RAM ÙˆØ§Ù‚Ø¹ÛŒ) Ø±Ùˆ Ø¯Ø± Ù†Ø¸Ø± Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ù‡. Ø§Ú¯Ø± Swap ÙØ¹Ø§Ù„ Ø¨Ø§Ø´Ù‡ØŒ ØªØµÙ…ÛŒÙ…â€ŒÚ¯ÛŒØ±ÛŒ Ø¯Ù‚ÛŒÙ‚ Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯ÛŒ Ø®Ø±Ø§Ø¨ Ù…ÛŒâ€ŒØ´Ù‡. Ø¨Ù†Ø§Ø¨Ø±Ø§ÛŒÙ† Ø¨Ø§ÛŒØ¯ Ù‚Ø¨Ù„ Ø§Ø² Ù†ØµØ¨ KubernetesØŒ Swap Ø±Ùˆ Ø®Ø§Ù…ÙˆØ´ Ú©Ù†ÛŒÙ…. ÙØ§ÛŒÙ„ /etc/fstab Ø±Ùˆ Ø¨Ø§ ÙˆÛŒØ±Ø§ÛŒØ´Ú¯Ø± Ø¨Ø§Ø² Ú©Ù† Ù…Ø«Ù„ nano ÛŒØ§ vim.

ğŸ”¹ 2 Ø±ÙˆØ´ Ø¨Ø±Ø§ÛŒ ØºÛŒØ±ÙØ¹Ø§Ù„ Ø³Ø§Ø²ÛŒ swap Ù‡Ø³Øª:

1.1. Ø±ÙˆØ´ Ø§ÙˆÙ„:
```bash
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
```
1.2. Ø±ÙˆØ´ Ø¯ÙˆÙ…:
```bash
sudo vim /etc/fstab
```
Ø¯Ø§Ø®Ù„ ÙØ§ÛŒÙ„ Ø®Ø· Ù…Ø´Ø§Ø¨Ù‡ Ø²ÛŒØ± Ø±Ùˆ Ù¾ÛŒØ¯Ø§ Ú©Ù†:

/swapfile          none          swap          sw          0          0

Ùˆ Ø§ÙˆÙ† Ø®Ø· Ø±Ùˆ Ø­Ø°Ù Ú©Ù†ØŒ Ø³Ù¾Ø³ Ø³ÛŒØ³ØªÙ… Ø±Ùˆ reboot Ú©Ù†.
ğŸ’¡ Ù†Ú©ØªÙ‡:

Ø¨Ø±Ø§ÛŒ proper Ø¨ÙˆØ¯Ù† kubeletØŒ swap Ø¨Ø§ÛŒØ¯ Ø±ÙˆÛŒ Ù‡Ø± Ø¯Ùˆ Ø³Ø±ÙˆØ± Master Ùˆ Worker Ø®Ø§Ù…ÙˆØ´ Ø¨Ø´Ù‡.

2ï¸âƒ£ ØªÙ†Ø¸ÛŒÙ… IPv4 Bridge Networking Ø¯Ø± ØªÙ…Ø§Ù… Ù†ÙˆØ¯Ù‡Ø§ ğŸŒ‰

Ø¨Ø±Ø§ÛŒ Ù…Ø´Ø§Ù‡Ø¯Ù‡ ØªØ±Ø§ÙÛŒÚ© bridged Ø¯Ø± iptablesØŒ ÛŒÚ© Ù…Ø§Ú˜ÙˆÙ„ Ùˆ ØªÙ†Ø¸ÛŒÙ… sysctl Ù†ÛŒØ§Ø² Ø¯Ø§Ø±ÛŒÙ….
ğŸ”¹ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø§Ú˜ÙˆÙ„â€ŒÙ‡Ø§:
```bash
cat <<EOF | sudo tee /etc/modules-load.d/containerd.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
```
Ø§ÛŒÙ† Ø¯Ùˆ Ù…Ø§Ú˜ÙˆÙ„ Ø¨Ø±Ø§ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ ØµØ­ÛŒØ­ networking Ú©Ø§Ù†ØªÛŒÙ†Ø±Ù‡Ø§ Ø§Ù„Ø²Ø§Ù…Ù†.
ğŸ”¹ ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ bridged traffic:
```bash
cat <<EOF | sudo tee /etc/sysctl.d/kubernetes.conf
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward = 1
EOF
```
ğŸ”¹ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¨Ø¯ÙˆÙ† Ø±ÛŒØ¨ÙˆØª:
```bash
sudo sysctl --system
```
3ï¸âƒ£ Ù†ØµØ¨ Container Runtime (Containerd) ğŸ³

Kubernetes requires a container runtime to manage containers.
ğŸ”¹ Ù†ØµØ¨ containerd:
```bash
sudo apt install containerd -y
```
ğŸ”¹ ØªÙ†Ø¸ÛŒÙ… ÙØ§ÛŒÙ„ Ú©Ø§Ù†ÙÛŒÚ¯:
```bash
sudo mkdir -p /etc/containerd
sudo containerd config default | sudo tee /etc/containerd/config.toml
```
ğŸ”¹ ÙˆÛŒØ±Ø§ÛŒØ´ ÙØ§ÛŒÙ„ Ø²ÛŒØ±:

Edit the following file:
```bash
sudo vim /etc/containerd/config.toml
```
Ø¯Ø± Ø¨Ø®Ø´ plugins... Ú¯Ø²ÛŒÙ†Ù‡ SystemdCgroup = false Ø±Ø§ Ø¨Ù‡ true ØªØºÛŒÛŒØ± Ø¨Ø¯Ù‡:
```
SystemdCgroup = true
```
ğŸ”¹ Ùˆ Ø¨Ø±Ø§ÛŒ Ø§Ø¹Ù…Ø§Ù„ ØªØºÛŒÛŒØ±Ø§Øª:
```bash
sudo systemctl restart containerd
```
4ï¸âƒ£ Ù†ØµØ¨ Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ÛŒ Kubernetes (kubeadm, kubelet, kubectl) ğŸ› ï¸
---
Letâ€™s install kubelet, kubeadm, and kubectl on each node to create a Kubernetes cluster. These components are essential for managing and operating a Kubernetes cluster.
---
ğŸ”¸ Kubeadm : The command to bootstrap the cluster.
---
ğŸ”¸ Kubelet : The component that runs on all machines in your cluster and starts pods and containers.
---
ğŸ”¸ Kubectl : The command-line utility to interact with your cluster.
---
âš ï¸ These instructions are for Kubernetes v1.33.
---
4.1 Ø¨Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù„ÛŒØ³Øª Ø¨Ø³ØªÙ‡â€ŒÙ‡Ø§:
```bash
sudo apt-get update
```
# apt-transport-https may be a dummy package; if so, you can skip that package
```bash
sudo apt-get install -y apt-transport-https ca-certificates curl gpg
```
4.2 Ø¯Ø±ÛŒØ§ÙØª Ú©Ù„ÛŒØ¯ Ø§Ù…Ø¶Ø§ÛŒ Ù¾Ú©ÛŒØ¬ Kubernetes:

# If the directory `/etc/apt/keyrings` does not exist, it should be created before the curl command, read the note below.
sudo mkdir -p -m 755 /etc/apt/keyrings
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.33/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg

ğŸ’¡ Note:

In releases older than Debian 12 and Ubuntu 22.04, directory /etc/apt/keyrings does not exist by default, and it should be created before the curl command.
4.3 Ø§ÙØ²ÙˆØ¯Ù† Ù…Ø®Ø²Ù† Kubernetes:

# This overwrites any existing configuration in /etc/apt/sources.list.d/kubernetes.list
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list

4.4 Ù†ØµØ¨ Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§:
sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl

4.5 ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ kubelet:
sudo systemctl enable --now kubelet

The kubelet is now restarting every few seconds, as it waits in a crashloop for kubeadm to tell it what to do.
4.6 ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ kubelet:

sudo systemctl enable kubelet

4.7. 4.7 Ù¾ÛŒØ´â€ŒÚ©Ø´ÛŒØ¯Ù† Ø§ÛŒÙ…ÛŒØ¬â€ŒÙ‡Ø§:

sudo kubeadm config images pull
4.8 Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø®ÙˆØ´Ù‡ (Ø±ÙˆÛŒ Master):
ğŸ’¡ Ø§Ø¨ØªØ¯Ø§ HAProxy Ø¨Ø§ÛŒØ¯ ØªÙ†Ø¸ÛŒÙ… Ø¨Ø§Ø´Ù‡! â—ï¸â—ï¸â—ï¸

kubeadm init --control-plane-endpoint "apisrv.aranetco.ir:8443" --pod-network-cidr=10.244.0.0/16 --upload-certs

âš ï¸ Ø§Ú¯Ø± Ú©Ù„Ø§Ø³ØªØ± Ú©Ø§Ø± Ù†Ú©Ø±Ø¯ Ø¨Ø±Ø§ÛŒ Ø±ÛŒØ³Øª :

sudo kubeadm reset --force

To manage the cluster, configure kubectl on the master node. Create the .kube directory in your home directory and copy the cluster's admin configuration to your personal .kube directory. Then, change the ownership of the copied configuration file to give the user permission to use it:

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

export KUBECONFIG=/etc/kubernetes/admin.conf

5ï¸âƒ£ Ù†ØµØ¨ Flannel ğŸŒ

Flannel is a simple and easy way to configure a layer 3 network fabric designed for Kubernetes.
ğŸ”¹ Deploy Flannel with kubectl:

kubectl apply -f https://github.com/coreos/flannel/raw/master/Documentation/kube-flannel.yml

âš ï¸ Warning:

If you use a custom podCIDR (not 10.244.0.0/16), you first need to download the above manifest and modify the network to match your configuration.
For example, if your custom podCIDR is 192.168.0.0/16, modify the network configuration in the downloaded manifest to match this range.

6ï¸âƒ£ ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ ØªÚ©Ù…ÛŒÙ„ Ø®ÙˆØ¯Ú©Ø§Ø± kubectl
BASH:
```bash
source <(kubectl completion bash)
echo "source <(kubectl completion bash)" >> ~/.bashrc
```
ZSH:
```bash
source <(kubectl completion zsh)
echo '[[ $commands[kubectl] ]] && source <(kubectl completion zsh)' >> ~/.zshrc
```
FISH:

For FISH shell, ensure you are using kubectl version 1.23 or above. Then, run the following command:
```bash
echo 'kubectl completion fish | source' > ~/.config/fish/completions/kubectl.fish && source ~/.config/fish/completions/kubectl.fish
```

7ï¸âƒ£ Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Worker Ø¨Ù‡ Ú©Ù„Ø§Ø³ØªØ± ğŸ¤
ğŸ”¹ Run the kubeadm join command on each worker node to connect it to the control plane (master node):

sudo kubeadm join [master-node-ip]:8443 --token [token] \
             --discovery-token-ca-cert-hash sha256:[hash]

ğŸš€ Deploying a Kubernetes Worker Node: Step-by-Step Guide

    ğŸš€ Prerequisites ğŸ› ï¸
    ğŸ›‘ Disable Swap Memory
    ğŸŒ‰ Set up the IPv4 Bridge Networking on All Nodes
    ğŸ³ Install Container Runtime (Containerd)
    ğŸ› ï¸ Install Kubernetes Tools (kubeadm, kubelet, kubectl)
    ğŸ¤ Join the Worker Node to the Cluster
    âœ… Verify the Node Addition
    ğŸŒ (Optional) Rebalance CoreDNS Pods
    ğŸ‰ Congratulations!

ğŸ’¡ Hint: This guide provides a detailed, step-by-step explanation for setting up and configuring a Kubernetes worker node from scratch. ğŸ‘‰ Click the title above or here to access the full guide.

8ï¸âƒ£ ØªÙ†Ø¸ÛŒÙ… HAProxy ğŸš€
ğŸ“ Ù…Ù‚Ø¯Ù…Ù‡

HAProxy, which stands for High Availability Proxy, is a popular open-source software solution for TCP/HTTP load balancing and proxying. It can run on Linux, macOS, and FreeBSD. Its primary purpose is to improve the performance and reliability of a server environment by distributing workloads across multiple servers (e.g., web, application, or database servers).

HAProxy is widely used in high-profile environments, including GitHub, Imgur, Instagram, and Twitter. ğŸŒ
Installation âœ”ï¸

Ù†ØµØ¨ HAProxy:
```bash
apt install haproxy
```
ÙˆÛŒØ±Ø§ÛŒØ´ /etc/haproxy/haproxy.cfg:
```bash
vim /etc/haproxy/haproxy.cfg
```
Below is the full configuration for HAProxy:
HAProxy Configuration File ğŸ› ï¸
```
# Frontend for Kubernetes API
frontend k8s-api
  bind *:8443
  mode tcp
  option tcplog
  default_backend k8s-api

# Backend for Kubernetes API
backend k8s-api
  mode tcp
  option tcplog
  option tcp-check
  balance roundrobin
  default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100
  server k8s-api-1 192.168.168.51:6443 check

# Monitoring HAProxy
frontend stats
  bind *:8404
  stats enable
  stats uri /stats
  stats refresh 10
```

Explanation of the Configuration File ğŸ“
1. Frontend for Kubernetes API

    frontend k8s-api: Defines a frontend named k8s-api to handle incoming traffic.
    bind *:8443: Binds the frontend to port 8443 on all available network interfaces. ğŸŒ
    mode tcp: Configures the frontend to operate in TCP mode (Layer 4), which is suitable for Kubernetes API traffic.
    option tcplog: Enables detailed logging for TCP connections. ğŸ“
    default_backend k8s-api: Specifies that traffic received on this frontend should be forwarded to the k8s-api backend.

2. Backend for Kubernetes API

    backend k8s-api: Defines a backend named k8s-api to handle traffic forwarded from the frontend.
    mode tcp: Configures the backend to operate in TCP mode.
    option tcplog: Enables detailed logging for TCP connections.
    option tcp-check: Enables health checks for backend servers using TCP. âœ…
    balance roundrobin: Distributes traffic evenly across all available servers in a round-robin fashion. ğŸ”„
    default-server: Sets default parameters for all servers in this backend:
        inter 10s: Interval between health checks is 10 seconds.
        downinter 5s: Interval between health checks for servers marked as down is 5 seconds.
        rise 2: A server is marked as healthy after 2 consecutive successful health checks.
        fall 2: A server is marked as unhealthy after 2 consecutive failed health checks.
        slowstart 60s: Gradually increases the server's weight over 60 seconds after it becomes healthy.
        maxconn 250: Limits the maximum number of concurrent connections to 250 per server.
        maxqueue 256: Limits the maximum number of queued connections to 256 per server.
        weight 100: Assigns a weight of 100 to the server for load balancing.
    server k8s-api-1 192.168.168.51:6443 check: Defines a backend server named k8s-api-1 with the IP address 192.168.168.51 and port 6443. The check option enables health checks for this server.

3. Monitoring HAProxy

    frontend stats: Defines a frontend named stats for monitoring HAProxy. ğŸ“Š
    bind *:8404: Binds the monitoring interface to port 8404 on all available network interfaces.
    stats enable: Enables the HAProxy statistics module.
    stats uri /stats: Specifies the URI path (/stats) for accessing the statistics page.
    stats refresh 10: Refreshes the statistics page every 10 seconds. ğŸ”„

9ï¸âƒ£ Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ Ø¢Ø¯Ø±Ø³ API Server

When setting up your Kubernetes cluster, it is essential to ensure that the API server address is properly resolved by all machines in the cluster, including the master and worker nodes. This can be achieved in one of two ways:
ğŸŒ 1. Ú¯Ø²ÛŒÙ†Ù‡ Ø§ÙˆÙ„: DNS Ø³Ø±ÙˆØ± (Recommended)
ÛŒÚ© Ø±Ú©ÙˆØ±Ø¯ Aâ€Œ Ø¨Ø³Ø§Ø² Ú©Ù‡ Ø¢Ø¯Ø±Ø³ Ø¯Ø§Ù…Ù†Ù‡â€ŒÛŒ API Ù…Ø«Ù„ api-server.example.com Ø±Ùˆ Ø¨Ù‡ IP Ø³Ø±ÙˆØ± API Ù†Ú¯Ø§Ø´Øª Ú©Ù†Ù‡.

If you have a DNS server configured, you should add the API server's address to the DNS records. This allows all nodes and clients in the cluster to resolve the API server's domain name to its IP address automatically.

For example, you would create a DNS record that maps the API server's domain name (e.g., api-server.example.com) to its IP address (e.g., 192.168.1.50).

âœ… Advantages of Using DNS:

    Centralized domain name resolution.
    Simplifies management, especially in larger or dynamic environments.

ğŸ–¥ï¸ 2.Ú¯Ø²ÛŒÙ†Ù‡ Ø¯ÙˆÙ…: ÙØ§ÛŒÙ„ /etc/hosts

If you do not have a DNS server, you must manually configure the API server's domain name resolution by adding an entry to the /etc/hosts file on all machines associated with the Kubernetes cluster, including the master and worker nodes.
Steps to Configure /etc/hosts:

Ø±ÙˆÛŒ Ù‡Ø± Ù†ÙˆØ¯ Ø®Ø· Ø²ÛŒØ± Ø±Ùˆ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†:
```bash
sudo vim /etc/hosts
```
Add the following line to map the API server's IP address to its domain name:
```
192.168.1.50 api-server.example.com
```
        Replace 192.168.1.50 with the IP address of your API server.
        Replace api-server.example.com with the desired domain name.

    Save and close the file.

â— Important Note

When you are not using a DNS server, you must include the domain name and API address of the API server in the /etc/hosts file on all machines in the Kubernetes cluster, including the master and worker nodes. This ensures that all nodes can resolve the API server's domain name to its IP address.
ğŸ“‹ Why This Step is Important

    DNS Server: Using a DNS server is the preferred method because it centralizes domain name resolution and simplifies management, especially in larger or dynamic environments.
    /etc/hosts File: This method is suitable for smaller setups or testing environments but requires manual updates on each machine if the API server's IP address changes.

ğŸ‰ Ù†ØªÛŒØ¬Ù‡â€ŒÚ¯ÛŒØ±ÛŒ
Ø­Ø§Ù„Ø§ HAProxy Ùˆ API Ø³Ø±ÙˆØ± Ø¨Ø§ Ù¾ÙˆØ±Øª 8443 ØªÙ†Ø¸ÛŒÙ… Ø´Ø¯Ù†ØŒ Kubernetes Ø¯Ø± Ù…Ø³ÛŒØ± Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù‡Ø³ØªØŒ Ùˆ Ø¢Ù…Ø§Ø¯Ù‡ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ù‡Ø³ØªÛŒ ğŸš€

ğŸŒŸ Why Use Port 8443 for HAProxy Instead of 6443 for the Kubernetes API Server?

When setting up a Kubernetes cluster with HAProxy as a load balancer, you may notice that the Kubernetes API server listens on port 6443 by default, but HAProxy is configured to listen on port 8443. This is a deliberate and important design choice. Here's why:
ğŸ”§ 1. Separation of Responsibilities

    Port 6443 is the default port used by the Kubernetes API server for secure communication.
    HAProxy acts as an intermediary (load balancer) between clients (e.g., kubectl, Kubernetes components, or external users) and the API server.
    To avoid confusion and clearly distinguish between direct API server access and load-balanced access, HAProxy is configured to listen on port 8443.

ğŸš« 2. Avoiding Port Conflicts

    If HAProxy were configured to listen on port 6443, it would conflict with the Kubernetes API server, which is already bound to that port on the master node(s).
    By using port 8443, we ensure that both services (HAProxy and the Kubernetes API server) can coexist without any port binding issues.

ğŸŒ 3. HAProxy as a Gateway

    HAProxy serves as a single entry point for all incoming traffic to the Kubernetes API server.
    By assigning it a different port (8443), it becomes clear that traffic on this port is being routed through the load balancer.
    This abstraction simplifies client configuration, as clients only need to know the HAProxy endpoint (e.g., https://haproxy.example.com:8443) rather than managing multiple API server endpoints.

ğŸ”’ 4. Security and Access Control

    Using a different port for HAProxy allows for better access control and firewall rules:
        Port 6443 can be restricted to internal components or administrators who need direct access to the API server.
        External users or clients can be directed to port 8443, where HAProxy handles load balancing and potentially additional security measures (e.g., rate limiting, logging).

âš–ï¸ 5. Flexibility in Multi-Master Setups

    In a multi-master Kubernetes cluster, HAProxy distributes traffic across multiple API servers running on different master nodes.
    By using port 8443, HAProxy provides a single, unified entry point for clients, while internally forwarding requests to the appropriate master node on port 6443.

ğŸ› ï¸ How It Works in Your Configuration

Hereâ€™s how this setup is reflected in your HAProxy configuration:

# Frontend for Kubernetes API
frontend k8s-api
  bind *:8443  # HAProxy listens on port 8443 for incoming traffic
  mode tcp
  option tcplog
  default_backend k8s-api

# Backend for Kubernetes API
backend k8s-api
  mode tcp
  option tcplog
  option tcp-check
  balance roundrobin
  server k8s-api-1 192.168.1.50:6443 check  # Forwards traffic to the API server on port 6443

    Frontend (bind *:8443): HAProxy listens for incoming traffic on port 8443.
    Backend (192.168.1.50:6443): HAProxy forwards the traffic to the Kubernetes API server running on port 6443.

This setup ensures that:

    Clients connect to HAProxy on port 8443.
    HAProxy distributes the traffic to the API server(s) on port 6443.

ğŸ¯ Key Benefits of Using Port 8443 for HAProxy

    No Port Conflicts: Avoids conflicts with the Kubernetes API server, which uses port 6443.
    Clear Separation: Distinguishes between direct API server access and load-balanced access.
    Simplified Access: Provides a single, unified entry point for clients.
    Enhanced Security: Allows for better access control and firewall rules.
    Scalability: Supports multi-master setups by routing traffic to multiple API servers.

ğŸ‰ Conclusion

Using port 8443 for HAProxy instead of port 6443 is a best practice that ensures a clean, scalable, and secure architecture for your Kubernetes cluster. It avoids conflicts, simplifies management, and provides a clear separation of responsibilities between HAProxy and the Kubernetes API server. ğŸš€
Author âœï¸
Created by Reza BND. If you find this repository helpful, feel free to fork it or contribute!
